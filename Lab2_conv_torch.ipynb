{"cells":[{"cell_type":"markdown","metadata":{"id":"GtOtsitDzLuh"},"source":["# CAM304\n","\n","## REDES NEURAIS E APRENDIZAGEM EM PROFUNDIDADE\n","\n","## Rede de Convolução / Visão Computacional (Versão PyTorch)\n","\n","### Prof. Eduardo Luz / Eduluz@ufop.edu.br\n","\n","Objetivos:\n","\n","- Aplicação de filtros em imagens por meio de convolução\n","- Entendimento do uso de stride, padding e pooling\n","- Modelagem de uma rede de convolução para o problema de rec. de face da AT&T\n","- Uso do VGG pré-treinado como um extrator de características\n","- Uso do MobileNet pré-treinado para classificação de faces : transferência de aprendizagem\n","- Notebook convertido para PyTorch.\n","\n","Data da entrega : a ser definido\n","\n","- Complete o código (marcado com ToDo) e quando requisitado, escreva textos diretamente nos notebooks. Onde tiver *None*, substitua pelo seu código.\n","- Crie um repositório no github para o grupo e me convide como colaborador (eduluzufop). Por fim, me envie o link do github.\n"]},{"cell_type":"markdown","metadata":{"id":"h6I8kas8Ib9l"},"source":["# 1. Aplicando filtros e entendendo padding, stride e pooling (20pt)"]},{"cell_type":"markdown","metadata":{"id":"ioIjMIjGtcXJ"},"source":["## 1.1. Importando pacotes e montando o drive"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"aiXCL7SaKjms","executionInfo":{"status":"ok","timestamp":1756929628571,"user_tz":180,"elapsed":21270,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models, transforms\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","\n","import os\n","import skimage\n","from skimage import io\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7LwHkqHbIGqX","outputId":"d18d6f93-dfc2-46ff-dc4d-c720dc6b0332","executionInfo":{"status":"ok","timestamp":1756929655633,"user_tz":180,"elapsed":27067,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gh_FZVJrtwjJ","executionInfo":{"status":"ok","timestamp":1756929655746,"user_tz":180,"elapsed":114,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["%matplotlib inline\n","from skimage.io import imread\n","from skimage.transform import resize"]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/aulas/CURSOS/ITV/CAM304-2025/CAM304_NeuralNet_Material/Práticas/datasets'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n9KeWJt4LBPb","executionInfo":{"status":"ok","timestamp":1756929738263,"user_tz":180,"elapsed":33,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}},"outputId":"cd100765-9db9-4617-cfb8-5ee51b07b321"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1PzA6OsAaMGsjswkhoUe2RXk2auC6B9oV/datasets\n"]}]},{"cell_type":"code","source":["%pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"H2Itvsr7LEC5","executionInfo":{"status":"ok","timestamp":1756929758581,"user_tz":180,"elapsed":91,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}},"outputId":"2381f523-1f32-4bca-a338-da1c5b7d194f"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/.shortcut-targets-by-id/1PzA6OsAaMGsjswkhoUe2RXk2auC6B9oV/datasets'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"Xxof7XPats1Z"},"source":["## 1.2. Carregando uma imagem"]},{"cell_type":"markdown","metadata":{"id":"750747gMta1B"},"source":["Carregue um imagem do disco, para usar como exemplo."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"0fFkIda6Ih7b","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1756929658384,"user_tz":180,"elapsed":2636,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}},"outputId":"7ec53b96-4004-4902-a000-d2d6bc6a57da"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"No such file: '/content/drive/My Drive/Colab Notebooks/Lenna.png'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-226951097.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# carrega imagem de exemplo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Substitua pelo seu caminho no Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msample_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Colab Notebooks/Lenna.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msample_image\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msample_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/skimage/_shared/utils.py\u001b[0m in \u001b[0;36mfixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeprecated_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodify_docstring\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hide_plugin_deprecation_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/skimage/_shared/utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mstacklevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stack_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstack_rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;31m# modify docstring to display deprecation warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Could not find the plugin \"{plugin}\" for {kind}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/skimage/io/_plugins/imageio_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WRITEABLE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/imageio/v3.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mcall_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/imageio/core/imopen.py\u001b[0m in \u001b[0;36mimopen\u001b[0;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_hint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_hint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"<bytes>\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, uri, mode, extension, format_hint, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Parse what was given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# Set extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# Reading: check that the file exists (but is allowed a dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file: '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;31m# Writing: check that the directory to write to does exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: No such file: '/content/drive/My Drive/Colab Notebooks/Lenna.png'"]}],"source":["# carrega imagem de exemplo\n","# Substitua pelo seu caminho no Drive\n","sample_image = imread(\"/content/drive/MyDrive/aulas/CURSOS/ITV/CAM304-2025/CAM304_NeuralNet_Material/Práticas/datasets/Lenna.png\")\n","sample_image= sample_image.astype(float)\n","\n","size = sample_image.shape\n","print(\"sample image shape: \", sample_image.shape)\n","\n","plt.imshow(sample_image.astype('uint8'));"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HzFhzZjQJMEM","executionInfo":{"status":"aborted","timestamp":1756929658461,"user_tz":180,"elapsed":14,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# veja o shape da imagem\n","print(sample_image.shape)"]},{"cell_type":"markdown","metadata":{"id":"e_bOePQnIlEG"},"source":["## 1.3. Criando e aplicando um filtro com convolução"]},{"cell_type":"markdown","metadata":{"id":"TzDvuzMmt9p6"},"source":["Utilize o PyTorch para aplicar o filtro. Observe que nesta etapa não há necessidade de treinamento algum. O código abaixo cria uma camada de convolução com 3 filtros de tamanho 5x5, e adiciona padding de forma a manter a imagem de saída (filtrada) do mesmo tamanho da imagem de entrada (padding =\"same\")."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9YNRZyiKv_b","executionInfo":{"status":"aborted","timestamp":1756929658487,"user_tz":180,"elapsed":51398,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# Cria uma camada de convolução nn.Conv2d.\n","# PyTorch espera entrada no formato (Batch, Canais, Altura, Largura).\n","# Nossa imagem é (Altura, Largura, Canais), então precisamos permutar as dimensões.\n","conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(5, 5), padding=\"same\")\n","\n","# Vamos verificar a camada\n","print(conv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHcUBhaaLN1S","executionInfo":{"status":"aborted","timestamp":1756929658488,"user_tz":180,"elapsed":51398,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# Para ver um sumário similar ao Keras, podemos usar a biblioteca torchinfo\n","!pip install -q torchinfo\n","from torchinfo import summary\n","summary(conv, input_size=(1, 3, 512, 512)) # (Batch, Canais, Altura, Largura)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Tw7QxGJJczf","executionInfo":{"status":"aborted","timestamp":1756929658490,"user_tz":180,"elapsed":51400,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# com PyTorch, as convoluções esperam tensores no formato : (batch_size, channels, height, width).\n","# Uma imagem isolada é considerada um lote de tamanho 1, portanto, deve-se adicionar uma dimensão de lote (batch) e\n","# permutar as dimensões de canais, altura e largura.\n","img_in = torch.from_numpy(sample_image).float()\n","img_in = img_in.permute(2, 0, 1) # (H, W, C) -> (C, H, W)\n","img_in = img_in.unsqueeze(0)    # (C, H, W) -> (B, C, H, W) com B=1\n","\n","print(img_in.shape)"]},{"cell_type":"markdown","metadata":{"id":"xQQZQXb4J6St"},"source":["Agora, pode-se aplicar a convolução. Aplique a convolução na imagem de exemplo e verifique o tamanho da imagem resultante (img_out)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5a3CfiytJ-CF","executionInfo":{"status":"aborted","timestamp":1756929658491,"user_tz":180,"elapsed":51401,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["img_out = conv(img_in)\n","print(img_out.shape)"]},{"cell_type":"markdown","metadata":{"id":"rB4nKTebLFCH"},"source":["Plote as imagens lado a lado e observe o resultado. O parâmetro `padding=\"same\"` aplica um padding automático no sentido de garantir que a saída tenha o mesmo tamanho da entrada.\n","Lembre-se que o padding adiciona zeros nas bordas da imagem, antes da aplicação da convolução."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkYAP2LxLHo-","executionInfo":{"status":"aborted","timestamp":1756929658492,"user_tz":180,"elapsed":51401,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# Para exibir, precisamos reverter as permutações e converter para numpy\n","img_out_display = img_out.squeeze(0).permute(1, 2, 0).detach().numpy()\n","\n","fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(sample_image.astype('uint8'))\n","ax0.set_title(\"Original\")\n","# Normalizar a saída para o intervalo [0, 255] para visualização correta\n","img_out_display_norm = (img_out_display - img_out_display.min()) / (img_out_display.max() - img_out_display.min())\n","ax1.imshow((img_out_display_norm * 255).astype('uint8'));\n","ax1.set_title(\"Filtrada com Padding 'same'\");"]},{"cell_type":"markdown","metadata":{"id":"GVtXvb-LLSi_"},"source":["Repita o mesmo procedimento, trocando padding de 'same' para 'valid' (que é padding=0 em PyTorch), usando apenas um filtro de saída."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFG2X1O3La2_","executionInfo":{"status":"aborted","timestamp":1756929658493,"user_tz":180,"elapsed":51401,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["conv2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=(5, 5), padding=\"valid\") # valid é o mesmo que padding=0\n","print(conv2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FycvJ_HbMOt1","executionInfo":{"status":"aborted","timestamp":1756929658494,"user_tz":180,"elapsed":51402,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["summary(conv2, input_size=(1, 3, 512, 512)) # 1 filtro 5x5x3 ... a profundidade do filtro é de acordo com a entrada. 5x5x3 = 75; Não esqueça do bias! Total: 75+1=76"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcCK91x6Li7s","executionInfo":{"status":"aborted","timestamp":1756929658496,"user_tz":180,"elapsed":51404,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["img_out = conv2(img_in)\n","print(img_out.shape)"]},{"cell_type":"markdown","metadata":{"id":"yS2S43lzL1xF"},"source":["Plote as duas imagens lado a lado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZbI6okrYLyop","executionInfo":{"status":"aborted","timestamp":1756929658499,"user_tz":180,"elapsed":51406,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["#  Como tivemos que expandir a primeira dimensao para aplicar a convolução, podemos remover a dimensão unitária para plotar a imagem, usando a função squeeze()\n","i = img_out.squeeze().detach().numpy() # squeeze remove todas as dimensões de tamanho 1\n","print(i.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKlpI-vKL4Kg","executionInfo":{"status":"aborted","timestamp":1756929658522,"user_tz":180,"elapsed":51429,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(sample_image.astype('uint8'))\n","ax0.set_title(\"Original\")\n","i = img_out.squeeze().detach().numpy()\n","ax1.imshow(i.astype('uint8'), cmap='gray'); # A saída tem apenas 1 canal, então usamos cmap gray\n","ax1.set_title(\"Filtrada com Padding 'valid'\");"]},{"cell_type":"markdown","metadata":{"id":"HcPKjTQXMWbj"},"source":["## 1.4. Inicializando os filtros na mão\n"]},{"cell_type":"markdown","metadata":{"id":"AvnWD6KJuQzC"},"source":["A função abaixo inicializa um tensor de dimensões (3, 3, 5, 5) com todas as posições zero, exceto as diagonais, que recebem o valor 1/25. Isso criará um filtro de média (borrão)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"orRVhpK8MlZs","executionInfo":{"status":"aborted","timestamp":1756929658524,"user_tz":180,"elapsed":51430,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["def my_filter_tensor():\n","    # Formato do peso em PyTorch: (out_channels, in_channels, kernel_height, kernel_width)\n","    shape = (3, 3, 5, 5)\n","    array = torch.zeros(shape, dtype=torch.float32)\n","    # Cria um filtro de média 5x5 para cada canal de entrada mapeando para seu canal de saída correspondente\n","    filter_kernel = torch.ones(5, 5) / 25.0\n","    array[0, 0, :, :] = filter_kernel # Canal 0 -> Canal 0\n","    array[1, 1, :, :] = filter_kernel # Canal 1 -> Canal 1\n","    array[2, 2, :, :] = filter_kernel # Canal 2 -> Canal 2\n","    return array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iO2WM7_BMoG_","executionInfo":{"status":"aborted","timestamp":1756929658537,"user_tz":180,"elapsed":51443,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# visualizando o filtro\n","print(my_filter_tensor()[0,0,:,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3jxSfVL_M1ap","executionInfo":{"status":"aborted","timestamp":1756929658538,"user_tz":180,"elapsed":51444,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# a função definida acima é usada para carregar valores nos filtros.\n","conv3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(5, 5), padding=\"same\", bias=False)\n","\n","# Carregando os pesos customizados na camada\n","with torch.no_grad():\n","    conv3.weight = nn.Parameter(my_filter_tensor())"]},{"cell_type":"markdown","metadata":{"id":"PQbsrU9nNVZZ"},"source":["## 1.5. Plote e observe o que acontece com a imagem (1pt)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Jnypi8EjugpR"},"source":["A imagem resultante ficou borrada (desfocada), como esperado de um filtro de média. Cada pixel na imagem de saída é a média dos pixels em uma vizinhança 5x5 na imagem original, o que suaviza as transições abruptas e reduz a nitidez."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iK9IyagzNYF0","executionInfo":{"status":"aborted","timestamp":1756929658540,"user_tz":180,"elapsed":51445,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# observe o que acontece com a imagem\n","img_out_3 = conv3(img_in)\n","img_out_3_display = img_out_3.squeeze(0).permute(1, 2, 0).detach().numpy()\n","\n","fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(sample_image.astype('uint8'))\n","ax0.set_title(\"Original\")\n","ax1.imshow(img_out_3_display.astype('uint8'));\n","ax1.set_title(\"Filtro de Média (Borrão)\");"]},{"cell_type":"markdown","metadata":{"id":"60eGKZowshsS"},"source":["### Responda\n","\n","**ToDo** : Descreva suas observações sobre a imagem anterior."]},{"cell_type":"markdown","metadata":{"id":"MAapLCWnu0t5"},"source":["## 1.6. Filtros de borda (5pt)"]},{"cell_type":"markdown","metadata":{"id":"tyeZPzgENmjJ"},"source":["**ToDo** : Crie uma nova função para gerar um filtro de borda nos 3 canais da imagem de entrada. O filtro deve ser 3x3 e ter o formato [[0, 0.2, 0], [0, -0.2, 0], [0, 0, 0]] (2pt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzYdQ0zwN4-f","executionInfo":{"status":"aborted","timestamp":1756929658541,"user_tz":180,"elapsed":51446,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["def my_new_filter_tensor(shape=(3, 3, 3, 3), dtype=None):\n","    # Formato do peso em PyTorch: (out_channels, in_channels, kernel_height, kernel_width)\n","    array = torch.zeros(shape, dtype=torch.float32)\n","\n","    # Filtro de borda vertical\n","    filter_kernel = torch.tensor([[0, 0.2, 0], [0, -0.2, 0], [0, 0, 0]], dtype=torch.float32)\n","\n","    # Aplica o mesmo filtro para cada canal\n","    for i in range(shape[0]): # out_channels\n","        for j in range(shape[1]): # in_channels\n","            if i == j: # Mapeia cada canal de entrada para o seu de saída correspondente\n","                array[i, j, :, :] = filter_kernel\n","\n","    return array"]},{"cell_type":"markdown","metadata":{"id":"zvxcaAm0OEBW"},"source":["Inicialize o objeto conv4 com seu novo filtro e aplique na imagem de entrada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2hbw4b8ODZS","executionInfo":{"status":"aborted","timestamp":1756929658542,"user_tz":180,"elapsed":51447,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["conv4 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(3, 3), padding=\"same\", bias=False)\n","\n","with torch.no_grad():\n","    conv4.weight = nn.Parameter(my_new_filter_tensor(shape=(3,3,3,3)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fio-DLbTOQxU","executionInfo":{"status":"aborted","timestamp":1756929658559,"user_tz":180,"elapsed":51463,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# Plote as duas imagens lado a lado (filtrada e não filtrada)\n","img_out_4 = conv4(img_in)\n","img_out_4_display = img_out_4.squeeze(0).permute(1, 2, 0).detach().numpy()\n","\n","fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(sample_image.astype('uint8'))\n","ax0.set_title(\"Original\")\n","# Normalizar para exibição\n","img_out_4_display_norm = (img_out_4_display - img_out_4_display.min()) / (img_out_4_display.max() - img_out_4_display.min())\n","ax1.imshow((img_out_4_display_norm * 255).astype('uint8'));\n","ax1.set_title(\"Filtro de Borda\")"]},{"cell_type":"markdown","metadata":{"id":"rWwPoQoaQVij"},"source":["## 1.7. Pooling (14pt)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AH4AZV5CvVUp"},"source":["Aplique um max-pooling na imagem, com uma janela de 2x2. Faça com stride de 2 e observe o resultado na imagem de saída."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xPxbeF4Qw6H","executionInfo":{"status":"aborted","timestamp":1756929658561,"user_tz":180,"elapsed":51465,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# cria um objeto de Max Pooling nn.MaxPool2d.\n","max_pool = nn.MaxPool2d(kernel_size=2, stride=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjewDi0CRHQp","executionInfo":{"status":"aborted","timestamp":1756929658563,"user_tz":180,"elapsed":51466,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["img_out = max_pool(img_in) # aplica o pooling\n","print(\"Shape de entrada:\", img_in.shape)\n","print(\"Shape de saída:\", img_out.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A9QJ2PdVR2xJ","executionInfo":{"status":"aborted","timestamp":1756929658565,"user_tz":180,"elapsed":51468,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# plota as imagens lado a lado\n","img_out_display = img_out.squeeze(0).permute(1, 2, 0).detach().numpy()\n","\n","fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(sample_image.astype('uint8'))\n","ax0.set_title(\"Original\")\n","ax1.imshow(img_out_display.astype('uint8'));\n","ax1.set_title(\"Após Max Pooling 2x2, Stride 2\")"]},{"cell_type":"markdown","metadata":{"id":"G_wEmcEtQsiV"},"source":["Aumente o stride para 4, repita o processo e observe o resultado na imagem de saída.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1h2QRKnSDmB","executionInfo":{"status":"aborted","timestamp":1756929658566,"user_tz":180,"elapsed":51469,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# cria um objeto de Max Pooling nn.MaxPool2d.\n","# Coloque o parametro stride para 4\n","max_pool2 = nn.MaxPool2d(kernel_size=2, stride=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HlUE0S0SSM32","executionInfo":{"status":"aborted","timestamp":1756929658567,"user_tz":180,"elapsed":51469,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["img_out = max_pool2(img_in) # aplica o pooling\n","print(\"Shape de entrada:\", img_in.shape)\n","print(\"Shape de saída:\", img_out.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8sBzHO0SfKW","executionInfo":{"status":"aborted","timestamp":1756929658568,"user_tz":180,"elapsed":51470,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# plota as imagens lado a lado\n","img_out_display = img_out.squeeze(0).permute(1, 2, 0).detach().numpy()\n","\n","fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(sample_image.astype('uint8'))\n","ax0.set_title(\"Original\")\n","ax1.imshow(img_out_display.astype('uint8'));\n","ax1.set_title(\"Após Max Pooling 2x2, Stride 4\")"]},{"cell_type":"markdown","metadata":{"id":"8JmfjDWCSW9Z"},"source":["Aumente o stride para 8, repita o processo e observe o resultado na imagem de saída."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYhwM9NASag7","executionInfo":{"status":"aborted","timestamp":1756929658569,"user_tz":180,"elapsed":51471,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# cria um objeto de Max Pooling nn.MaxPool2d.\n","# Coloque o parametro stride para 8\n","max_pool3 = nn.MaxPool2d(kernel_size=2, stride=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8MQ9af77ScoP","executionInfo":{"status":"aborted","timestamp":1756929658570,"user_tz":180,"elapsed":51471,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["img_out = max_pool3(img_in) # aplica o pooling\n","print(\"Shape de entrada:\", img_in.shape)\n","print(\"Shape de saída:\", img_out.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLfuHl3nSgqd","executionInfo":{"status":"aborted","timestamp":1756929658571,"user_tz":180,"elapsed":51472,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# plota as imagens lado a lado\n","img_out_display = img_out.squeeze(0).permute(1, 2, 0).detach().numpy()\n","\n","fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n","ax0.imshow(sample_image.astype('uint8'))\n","ax0.set_title(\"Original\")\n","ax1.imshow(img_out_display.astype('uint8'));\n","ax1.set_title(\"Após Max Pooling 2x2, Stride 8\")"]},{"cell_type":"markdown","metadata":{"id":"3EC3cQ8nSmUO"},"source":["### Responda\n","\n","**ToDo** - Descreva o que aconteceu com o aumento do stride.\n","\n","**Resposta:** O stride determina o passo com que a janela de pooling se move sobre a imagem. Um stride=2 significa que a janela pula 2 pixels de cada vez, resultando em uma imagem com metade da altura e largura. Conforme aumentamos o stride para 4 e depois para 8, o passo do pulo aumenta, resultando em uma redução cada vez maior no tamanho da imagem de saída. Isso causa uma perda significativa de informação espacial e a imagem fica cada vez mais pixelada e irreconhecível. O pooling com stride grande é uma forma agressiva de reduzir a dimensionalidade da imagem."]},{"cell_type":"markdown","metadata":{"id":"p7_CY-tWTECX"},"source":["# 2. Reconhecimento de Faces usando uma rede de convolução (20pt)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RIcj3otbv68h"},"source":["O objetivo desta etapa é classificar faces na base ORL (AT&T) Database (40 individuos x 10 imagens, de resolução 92x112 pixels e 256 níveis de cinza).\n","\n","Baixe as imagens no site http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html ou da pasta dataset do Drive."]},{"cell_type":"markdown","metadata":{"id":"2V1xdKA3wOl4"},"source":["## 2.1. Preparando os dados (5pt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZQn5ybsTaCC","executionInfo":{"status":"aborted","timestamp":1756929658572,"user_tz":180,"elapsed":51473,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# carregue as imagens\n","# inicializa matrizes X e y\n","X = np.empty([400, 112, 92], dtype=np.float32) # 40 classes com 10 imgs cada\n","y = np.empty([400], dtype=np.longlong) # y deve ser do tipo long para o CrossEntropyLoss\n","\n","# percorre todos os diretorios da base att e carrega as imagens\n","# Substitua pelo seu caminho no Drive\n","imgs_path = \"/content/drive/My Drive/Colab Notebooks/datasets/AttFaces\"\n","i=0\n","class_id = 0\n","sorted_dirs = sorted(os.listdir(imgs_path))\n","for f in sorted_dirs:\n","    if f.startswith(\"s\"):\n","        # O rótulo da classe será de 0 a 39\n","        class_label = int(f.replace(\"s\", \"\")) - 1\n","        subject_path = os.path.join(imgs_path, f)\n","        sorted_imgs = sorted(os.listdir(subject_path))\n","        for img_path in sorted_imgs:\n","            if img_path.endswith(\".pgm\"):\n","                X[i, :, :] = io.imread(os.path.join(subject_path, img_path))\n","                y[i] = class_label\n","                i = i + 1\n","\n","print(\"dimensões da matriz X = \" , X.shape)\n","print(\"dimensões do vetor y = \" , y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJ2aX8xoTm5i","executionInfo":{"status":"aborted","timestamp":1756929658575,"user_tz":180,"elapsed":51475,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# Divida os dados em treino e teste (70%-30%) com a função train_test_split\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKryDUQBVYnh","executionInfo":{"status":"aborted","timestamp":1756929658576,"user_tz":180,"elapsed":51476,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["print(X_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVtK6BmVVai2","executionInfo":{"status":"aborted","timestamp":1756929658578,"user_tz":180,"elapsed":51478,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["print(X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"Q_9_N7bIwXBB"},"source":["## 2.2. Implementando a rede (15pt)"]},{"cell_type":"markdown","metadata":{"id":"AvVOLXRnT7Yp"},"source":["Implemente uma rede de convolução simples, contendo 3 camadas de convolução seguidas de camadas max-pooling. Duas camadas densas (totalmente conectadas) no final e por fim uma camada com ativação softmax para a classificação. Escolha filtros de tamanhos variados : (3,3) ou (5,5). Para cada camada, crie de 32 a 96 filtros.\n","Na camada densa, use de 64 a 200 neurônios.\n","\n","Use o comando model.summary() para conferir a arquitetura."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AuyzOL8_Udir","executionInfo":{"status":"aborted","timestamp":1756929658579,"user_tz":180,"elapsed":51478,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# Implementa uma rede de convolução simples, como uma classe nn.Module\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv_layers = nn.Sequential(\n","            # Bloco 1\n","            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Bloco 2\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Bloco 3\n","            nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.flatten = nn.Flatten()\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(96 * 14 * 11, 128), # A dimensão de entrada precisa ser calculada\n","            nn.ReLU(),\n","            nn.Linear(128, 40) # 40 classes de saída (não precisa de Softmax aqui)\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)\n","        x = self.flatten(x)\n","        logits = self.fc_layers(x)\n","        return logits\n","\n","model = SimpleCNN()\n","# O input_size para o summary é (Batch, Canais, Altura, Largura)\n","summary(model, input_size=(1, 1, 112, 92))"]},{"cell_type":"markdown","metadata":{"id":"_UxtfYFxVF7F"},"source":["A saída acima mostra uma arquitetura similar à solicitada. Note que os nomes das camadas e o número de parâmetros podem variar ligeiramente, mas a estrutura (Conv-Pool-Conv-Pool-Conv-Pool-Flatten-Dense-Dense) é a mesma."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-1bgTXLVjxo","executionInfo":{"status":"aborted","timestamp":1756929658592,"user_tz":180,"elapsed":51491,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# O shape de X_train é (N, H, W). PyTorch precisa de (N, C, H, W).\n","# Como a imagem é em escala de cinza, adicionaremos uma dimensão de canal (C=1).\n","print(\"Shape original:\", X_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KfF5fxITV1VK","executionInfo":{"status":"aborted","timestamp":1756929658594,"user_tz":180,"elapsed":51492,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# Adiciona a dimensão de canal e converte para tensores PyTorch\n","X_train_tensor = torch.from_numpy(np.expand_dims(X_train, axis=1)).float()\n","X_test_tensor = torch.from_numpy(np.expand_dims(X_test, axis=1)).float()\n","y_train_tensor = torch.from_numpy(y_train).long()\n","y_test_tensor = torch.from_numpy(y_test).long()\n","\n","print(\"Shape do tensor de treino:\", X_train_tensor.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwWcz94_Wg_Y","executionInfo":{"status":"aborted","timestamp":1756929658596,"user_tz":180,"elapsed":51494,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# Cria os datasets e dataloaders\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","\n","train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"2gAGaOloWqYe"},"source":["Defina a função de custo (loss) = CrossEntropyLoss e o método de otimização=Adam."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INfxKh-dW1kb","executionInfo":{"status":"aborted","timestamp":1756929658597,"user_tz":180,"elapsed":51495,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","metadata":{"id":"7kpuR9ASW3yY"},"source":["Treine o modelo por 30 épocas com batch_size = 100."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKBagOnQW8UY","executionInfo":{"status":"aborted","timestamp":1756929658598,"user_tz":180,"elapsed":51495,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["epochs = 30\n","history = {'loss': [], 'val_loss': [], 'acc': [], 'val_acc': []}\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_train += labels.size(0)\n","        correct_train += (predicted == labels).sum().item()\n","\n","    # Validação\n","    model.eval()\n","    val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss_val = criterion(outputs, labels)\n","            val_loss += loss_val.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_val += labels.size(0)\n","            correct_val += (predicted == labels).sum().item()\n","\n","    # Salva o histórico\n","    history['loss'].append(running_loss / len(train_loader))\n","    history['val_loss'].append(val_loss / len(test_loader))\n","    history['acc'].append(100 * correct_train / total_train)\n","    history['val_acc'].append(100 * correct_val / total_val)\n","\n","    print(f\"Época [{epoch+1}/{epochs}] | Treino Loss: {history['loss'][-1]:.4f} | Treino Acc: {history['acc'][-1]:.2f}% | Val Loss: {history['val_loss'][-1]:.4f} | Val Acc: {history['val_acc'][-1]:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"g4eFFq1UXCgW"},"source":["O objeto history foi criado para armazenar o histórico do treino."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jO1qNLbTXLJ5","executionInfo":{"status":"aborted","timestamp":1756929658601,"user_tz":180,"elapsed":51498,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["print(history.keys())"]},{"cell_type":"markdown","metadata":{"id":"GvC_mcoqXOms"},"source":["Plote a acurácia e o custo (loss) do treino e da validação."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3Xf6Q4mXRbT","executionInfo":{"status":"aborted","timestamp":1756929658602,"user_tz":180,"elapsed":51499,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","\n","ax1.plot(history['acc'], label='Acurácia de Treino')\n","ax1.plot(history['val_acc'], label = 'Acurácia de Validação')\n","ax1.set_xlabel('Época')\n","ax1.set_ylabel('Acurácia')\n","ax1.legend(loc='lower right')\n","ax1.set_title('Acurácia por Época')\n","\n","ax2.plot(history['loss'], label='Perda de Treino')\n","ax2.plot(history['val_loss'], label = 'Perda de Validação')\n","ax2.set_xlabel('Época')\n","ax2.set_ylabel('Perda')\n","ax2.legend(loc='upper right')\n","ax2.set_title('Perda por Época')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqZ-m0sgXU9D","executionInfo":{"status":"aborted","timestamp":1756929658603,"user_tz":180,"elapsed":51499,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["test_acc = history['val_acc'][-1]\n","print(f\"Acurácia final no conjunto de teste: {test_acc:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"THZPjbihXYpq"},"source":["# 3. Usando um modelo Pré-treinado : VGG (10pt)"]},{"cell_type":"markdown","metadata":{"id":"fLwTl8v6Xo-T"},"source":["Carregando os dados da base AT&T para o VGG. Como a base está em escala de cinza e a entrada do modelo VGG espera uma imagem colorida (RGB), vamos repetir a mesma imagem em cada uma das bandas."]},{"cell_type":"markdown","metadata":{"id":"7I2aT3rcwpYI"},"source":["## 3.1. Preparando os dados (2pt)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnpSaR3sXs6z","executionInfo":{"status":"aborted","timestamp":1756929658604,"user_tz":180,"elapsed":51500,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# inicializa matrizes X e y\n","X_color = np.empty([400, 112, 92, 3], dtype=np.uint8) # Formato de imagem para transforms\n","y_color = np.empty([400], dtype=np.longlong)\n","\n","# Reutilizando o loop anterior para carregar as imagens\n","i=0\n","for f in sorted_dirs:\n","    if f.startswith(\"s\"):\n","        class_label = int(f.replace(\"s\", \"\")) - 1\n","        subject_path = os.path.join(imgs_path, f)\n","        sorted_imgs = sorted(os.listdir(subject_path))\n","        for img_path in sorted_imgs:\n","            if img_path.endswith(\".pgm\"):\n","                img_gray = io.imread(os.path.join(subject_path, img_path))\n","                # Converte imagem cinza para RGB repetindo o canal\n","                X_color[i, :, :, :] = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)\n","                y_color[i] = class_label\n","                i = i + 1\n","\n","print(\"Shape de X_color:\", X_color.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-7f5k_KYD1K","executionInfo":{"status":"aborted","timestamp":1756929658605,"user_tz":180,"elapsed":51500,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# divida em 70% treino e 30% teste\n","from sklearn.model_selection import train_test_split\n","X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_color, y_color, test_size=0.3, random_state=42, stratify=y_color)\n","print(X_train_c.shape)"]},{"cell_type":"markdown","metadata":{"id":"xdwp1AqgYNn5"},"source":["## 3.2. Carregando o VGG direto da biblioteca do torchvision (2pt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXVTuPpHYQ_6","executionInfo":{"status":"aborted","timestamp":1756929658607,"user_tz":180,"elapsed":51502,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["from torchvision.models import vgg19, VGG19_Weights\n","\n","vgg19 = vgg19(weights=VGG19_Weights.DEFAULT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMiaPJ6wYUIz","executionInfo":{"status":"aborted","timestamp":1756929658609,"user_tz":180,"elapsed":51504,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["summary(vgg19, input_size=(1, 3, 224, 224)) # repare a quantidade de parâmetros!"]},{"cell_type":"markdown","metadata":{"id":"SDY6510wYb-z"},"source":["Vamos descartar as duas últimas camadas do VGG (a última camada de ativação ReLU e a camada de classificação final) para usar a penúltima camada densa como nosso descritor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAaisGc9Yfnu","executionInfo":{"status":"aborted","timestamp":1756929658621,"user_tz":180,"elapsed":51515,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["vgg_face_descriptor = nn.Sequential(*list(vgg19.children())[:-1], # Remove a parte do classificador\n","                                  nn.Flatten(), # Adiciona flatten\n","                                  *list(vgg19.classifier.children())[:-1]) # Pega todas as camadas do classificador, exceto a última\n","\n","vgg_face_descriptor.eval() # Coloca em modo de avaliação\n","vgg_face_descriptor.to(device)\n","print(vgg_face_descriptor)"]},{"cell_type":"markdown","metadata":{"id":"at9gD07JYg0P"},"source":["### Responda\n","\n","**ToDo** - Por que descartamos as duas últimas camadas do VGG?\n","\n","**Resposta:** Descartamos as últimas camadas porque elas são específicas para a tarefa original na qual o VGG foi treinado (classificação de 1000 classes do ImageNet). A penúltima camada densa (antes da camada de classificação final) contém uma representação rica e de alto nível das características da imagem. Ao remover a camada final, usamos a saída dessa penúltima camada como um \"descritor de características\" ou \"embedding\" genérico, que pode ser usado para outras tarefas, como medir a similaridade entre faces."]},{"cell_type":"markdown","metadata":{"id":"_wJwUqOIYrsF"},"source":["## 3.3 Medindo Similaridade"]},{"cell_type":"markdown","metadata":{"id":"-MNTsud5xIE4"},"source":["### As funções abaixo servem para medir similaridade entre duas imagens, passando-se um vetor de características."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWa2vfOzYvzn","executionInfo":{"status":"aborted","timestamp":1756929658622,"user_tz":180,"elapsed":51516,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["def findCosineSimilarity(source_representation, test_representation):\n","    a = np.matmul(np.transpose(source_representation), test_representation)\n","    b = np.sum(np.multiply(source_representation, source_representation))\n","    c = np.sum(np.multiply(test_representation, test_representation))\n","    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n","\n","def findEuclideanDistance(source_representation, test_representation):\n","    euclidean_distance = source_representation - test_representation\n","    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n","    euclidean_distance = np.sqrt(euclidean_distance)\n","    return euclidean_distance"]},{"cell_type":"markdown","metadata":{"id":"Z0bCJE4IZTGh"},"source":["### A função verifyFace recebe duas imagens e calcula a similaridade entre elas. Se a similaridade for menor que epsilon, afirma-se que as duas imagens são de uma mesma pessoa."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XuYzucEcY1U8","executionInfo":{"status":"aborted","timestamp":1756929658623,"user_tz":180,"elapsed":51517,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["epsilon = 0.4 # Limiar ajustado para a distância do cosseno (1 - similaridade)\n","\n","# Definindo as transformações necessárias para o VGG\n","vgg_transforms = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","def verifyFace(img1_np, img2_np):\n","    # Aplica as transformações\n","    img1 = vgg_transforms(img1_np).unsqueeze(0).to(device)\n","    img2 = vgg_transforms(img2_np).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        img1_representation = vgg_face_descriptor(img1).cpu().numpy().flatten()\n","        img2_representation = vgg_face_descriptor(img2).cpu().numpy().flatten()\n","\n","    cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n","    euclidean_distance = findEuclideanDistance(img1_representation, img2_representation)\n","\n","    print(f\"Distância do Cosseno: {cosine_similarity:.4f}\")\n","    print(f\"Distância Euclidiana: {euclidean_distance:.4f}\")\n","\n","    if(cosine_similarity < epsilon):\n","        print(\"Verificado! Mesma pessoa!\")\n","    else:\n","        print(\"Não-verificado! Não são a mesma pessoa!\")\n","\n","    f, (ax1, ax2) = plt.subplots(1, 2)\n","    ax1.imshow(img1_np)\n","    ax1.set_xticks([]); ax1.set_yticks([])\n","    ax2.imshow(img2_np)\n","    ax2.set_xticks([]); ax2.set_yticks([])\n","    plt.show(block=True)\n","    print(\"-----------------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"w9lgb2ayyES4"},"source":["### Verificando a similaridade entre imagens (6pt)"]},{"cell_type":"markdown","metadata":{"id":"Wk2xKaknZboC"},"source":["Use 4 pares de imagens da base da AT&T e faça uma verificação entre elas, chamando a função verifyFace().\n","\n","A função `verifyFace` já aplica a normalização e o redimensionamento necessários.\n","\n","Faça para os pares de índices: (64 e 33), (3 e 7), (40 e 44), (100 e 200)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHGlWALSY40r","executionInfo":{"status":"aborted","timestamp":1756929658624,"user_tz":180,"elapsed":51517,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["pares = [(64, 33), (3, 7), (40, 44), (100, 200)]\n","\n","for i1, i2 in pares:\n","    img1 = X_color[i1]\n","    img2 = X_color[i2]\n","    label1 = y_color[i1]\n","    label2 = y_color[i2]\n","    print(f\"Comparando imagem {i1} (pessoa {label1}) com imagem {i2} (pessoa {label2})\")\n","    verifyFace(img1, img2)\n","\n","# Teste com duas imagens da mesma pessoa\n","print(\"Comparando duas imagens da mesma pessoa (índices 40 e 41)\")\n","verifyFace(X_color[40], X_color[41])"]},{"cell_type":"markdown","metadata":{"id":"bmJgumuzdzpu"},"source":["# 4. Transferência de aprendizado (50pt)\n"]},{"cell_type":"markdown","metadata":{"id":"HRUSJrfayPyI"},"source":["Estude o tutorial do [link](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) e aplique o mesmo procedimento para ajustar um modelo previamente treinado com imagens da imagenet.\n","Use o MobileNetV2 como modelo base.\n","\n","Todo: Faça o procedimento em duas etapas:\n","\n","\n","1.   Congele todas as camadas exceto as novas que você adicionou ao modelo. Treine.\n","2.   Libere todas as camadas para o treinamento e treine novamente com um Learning Rate bem pequeno (um décimo do realizado no item 1)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zBOd8fDNd5Kz","executionInfo":{"status":"aborted","timestamp":1756929658625,"user_tz":180,"elapsed":51518,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# Usando o MobileNet, as imagens devem ter entrada de 224x224x3 e normalizadas.\n","IMG_SIZE = 224\n","\n","# Criaremos um Dataset customizado para aplicar as transformações\n","from torch.utils.data import Dataset\n","\n","class FaceDataset(Dataset):\n","    def __init__(self, images, labels, transform=None):\n","        self.images = images\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        label = self.labels[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","# Definindo as transformações\n","train_transforms = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.RandomHorizontalFlip(), # Data Augmentation\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","test_transforms = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9evGRp6Jeapk","executionInfo":{"status":"aborted","timestamp":1756929658626,"user_tz":180,"elapsed":51519,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["# Criando os Datasets\n","train_dataset_tl = FaceDataset(X_train_c, y_train_c, transform=train_transforms)\n","test_dataset_tl = FaceDataset(X_test_c, y_test_c, transform=test_transforms)\n","\n","# Reservando 10% do treino para validação\n","val_size = int(0.1 * len(train_dataset_tl))\n","train_size = len(train_dataset_tl) - val_size\n","train_subset, val_subset = random_split(train_dataset_tl, [train_size, val_size])\n","\n","# Criando os DataLoaders\n","BATCH_SIZE = 32\n","train_loader_tl = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader_tl = DataLoader(val_subset, batch_size=BATCH_SIZE)\n","test_loader_tl = DataLoader(test_dataset_tl, batch_size=BATCH_SIZE)\n","\n","print(f\"Tamanho Treino: {len(train_subset)}, Validação: {len(val_subset)}, Teste: {len(test_dataset_tl)}\")"]},{"cell_type":"markdown","metadata":{"id":"2d83DOcdfDw_"},"source":["## 4.1. Execute os passos (35pt):\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6ZZrM-i7OOVT"},"source":["1. Carregue o modelo pré-treinado do MobileNet, remova a última camada.\n","2. Congele as camadas do modelo base.\n","3. Adicione uma nova camada de classificação.\n","4. Use função de custo `CrossEntropyLoss` e otimizador Adam.\n","5. Dentre os dados de treinamento, reserve 10% para validação do modelo (feito acima).\n","6. Treine por 10 épocas.\n","7. Plote os gráficos de custo do treino e validação\n","8. Descongele o modelo e treine por mais 10 épocas com um learning rate menor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qb3rfq47yWNf","executionInfo":{"status":"aborted","timestamp":1756929658632,"user_tz":180,"elapsed":51524,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}},"outputs":[],"source":["from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n","\n","# 1. Carregar o modelo e congelar as camadas\n","model_tl = mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n","for param in model_tl.parameters():\n","    param.requires_grad = False\n","\n","# 2. Substituir o classificador\n","num_classes = 40\n","in_features = model_tl.classifier[1].in_features\n","model_tl.classifier[1] = nn.Linear(in_features, num_classes)\n","\n","model_tl.to(device)\n","summary(model_tl, input_size=(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE))\n","\n","# 3. Definir loss, optimizer e treinar\n","criterion_tl = nn.CrossEntropyLoss()\n","# Otimizar apenas os parâmetros do novo classificador\n","optimizer_tl = torch.optim.Adam(model_tl.classifier.parameters(), lr=0.001)\n","\n","print(\"--- Treinando apenas a camada de classificação ---\")\n","epochs = 10\n","for epoch in range(epochs):\n","    model_tl.train()\n","    for inputs, labels in train_loader_tl:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer_tl.zero_grad()\n","        outputs = model_tl(inputs)\n","        loss = criterion_tl(outputs, labels)\n","        loss.backward()\n","        optimizer_tl.step()\n","    print(f\"Época [{epoch+1}/{epochs}], Perda: {loss.item():.4f}\")\n","\n","# 4. Descongelar e treinar novamente (fine-tuning)\n","for param in model_tl.parameters():\n","    param.requires_grad = True\n","\n","# Otimizador para todas as camadas com learning rate baixo\n","optimizer_tl = torch.optim.Adam(model_tl.parameters(), lr=0.0001)\n","\n","print(\"\\n--- Fine-tuning de todo o modelo ---\")\n","for epoch in range(epochs):\n","    model_tl.train()\n","    for inputs, labels in train_loader_tl:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer_tl.zero_grad()\n","        outputs = model_tl(inputs)\n","        loss = criterion_tl(outputs, labels)\n","        loss.backward()\n","        optimizer_tl.step()\n","    print(f\"Época [{epoch+1+10}/{epochs+10}], Perda: {loss.item():.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"xomY16Xuz_1_"},"source":["## 4.2. Fazendo testes (13pt)"]},{"cell_type":"markdown","metadata":{"id":"p5wzK5IfgWHu"},"source":["Analise os resultados. Você provavelmente deve ter observado overfitting.\n","Todo: Aplique algumas regularizações no modelo, para tentar reduzir o super-ajuste.Tente as opções abaixo:\n","\n","1.   Dropout, antes da camada densa, de 50%\n","2.   Regularização nos pesos da camada densa (L2, via `weight_decay` no otimizador)\n","\n","Veja exemplos na documentação do PyTorch."]},{"cell_type":"code","source":["# Modelo com regularização\n","model_reg = mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n","for param in model_reg.parameters():\n","    param.requires_grad = False\n","\n","# Substituir o classificador com Dropout\n","in_features_reg = model_reg.classifier[1].in_features\n","model_reg.classifier = nn.Sequential(\n","    nn.Dropout(p=0.5),\n","    nn.Linear(in_features_reg, num_classes)\n",")\n","\n","model_reg.to(device)\n","\n","# Otimizador com L2 (weight_decay)\n","optimizer_reg = torch.optim.Adam(model_reg.classifier.parameters(), lr=0.001, weight_decay=1e-4)\n","criterion_reg = nn.CrossEntropyLoss()\n","\n","print(\"--- Treinando com regularização (Dropout + L2) ---\")\n","epochs = 20 # Treinar por mais tempo para ver o efeito\n","for epoch in range(epochs):\n","    model_reg.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader_tl:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer_reg.zero_grad()\n","        outputs = model_reg(inputs)\n","        loss = criterion_reg(outputs, labels)\n","        loss.backward()\n","        optimizer_reg.step()\n","        running_loss += loss.item()\n","\n","    # Avaliação na validação para checar overfitting\n","    model_reg.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader_tl:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model_reg(inputs)\n","            val_loss += criterion_reg(outputs, labels).item()\n","            _, preds = torch.max(outputs, 1)\n","            correct += torch.sum(preds == labels.data)\n","    val_acc = correct.double() / len(val_subset)\n","    print(f\"Época [{epoch+1}/{epochs}] | Treino Loss: {running_loss/len(train_loader_tl):.4f} | Val Loss: {val_loss/len(val_loader_tl):.4f} | Val Acc: {val_acc:.4f}\")"],"execution_count":null,"outputs":[],"metadata":{"id":"overfitting_solution","executionInfo":{"status":"aborted","timestamp":1756929658633,"user_tz":180,"elapsed":51525,"user":{"displayName":"EDUARDO JOSE DA SILVA LUZ","userId":"04694447128035841155"}}}},{"cell_type":"markdown","metadata":{"id":"e5pM1K_Jg_pk"},"source":["### Responda (2pt)\n","\n","**ToDo** - com qual configuração conseguiu resolver o overfitting?\n","\n","**Resposta:** O overfitting em datasets pequenos como o AT&T Faces é comum. A combinação de técnicas de regularização geralmente produz os melhores resultados. Neste caso, a adição de uma camada de **Dropout** com p=0.5 antes da camada linear final e a inclusão de **decaimento de peso (L2)** no otimizador (ex: `weight_decay=1e-4`) ajudam a mitigar o overfitting. O Dropout desativa aleatoriamente neurônios durante o treinamento, forçando a rede a aprender características mais robustas. O decaimento de peso penaliza pesos grandes, prevenindo que o modelo se ajuste demais ao ruído nos dados de treino. A combinação de ambos é uma estratégia eficaz para melhorar a generalização do modelo."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}